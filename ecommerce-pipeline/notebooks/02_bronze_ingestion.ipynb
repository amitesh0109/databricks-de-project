{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e04a3287-290e-44d5-933a-531812b07c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bronze Layer - Data Ingestion (Azure Storage)\n",
    "\n",
    "# Define the STORAGE_ACCOUNT variable\n",
    "STORAGE_ACCOUNT = None\n",
    "\n",
    "# Get task values from data generation\n",
    "try:\n",
    "    expected_customers = dbutils.jobs.taskValues.get(taskKey=\"data_generation\", key=\"customers_generated\", debugValue=500)\n",
    "    expected_products = dbutils.jobs.taskValues.get(taskKey=\"data_generation\", key=\"products_generated\", debugValue=100)\n",
    "    expected_orders = dbutils.jobs.taskValues.get(taskKey=\"data_generation\", key=\"orders_generated\", debugValue=1000)\n",
    "    expected_order_items = dbutils.jobs.taskValues.get(taskKey=\"data_generation\", key=\"order_items_generated\", debugValue=2000)\n",
    "    source_path_from_generation = dbutils.jobs.taskValues.get(taskKey=\"data_generation\", key=\"source_path\")\n",
    "    STORAGE_ACCOUNT = dbutils.jobs.taskValues.get(taskKey=\"data_generation\", key=\"STORAGE_ACCOUNT\", debugValue=\"dataworks\")\n",
    "    \n",
    "    print(f\"üìã Expected data volumes from generation:\")\n",
    "    print(f\"   Customers: {expected_customers}\")\n",
    "    print(f\"   Products: {expected_products}\")\n",
    "    print(f\"   Orders: {expected_orders}\")\n",
    "    print(f\"   Order Items: {expected_order_items}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not retrieve task values (running standalone): {e}\")\n",
    "    expected_customers = expected_products = expected_orders = expected_order_items = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56ea97c6-c1d0-47bc-8050-7860d747a4ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Storage Configuration\n",
    "# Authentication \n",
    "spark.conf.set(f\"fs.azure.account.key.dataworks.dfs.core.windows.net\", \"access-Key\")\n",
    "\n",
    "print(\"Starting Bronze Layer with Azure Storage...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c6ad0f-319b-497b-81ea-1d2a2cf2dfb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Azure Storage Path Configuration\n",
    "CONFIG = {\n",
    "    'source_path': f'abfss://bronze@{STORAGE_ACCOUNT}.dfs.core.windows.net/raw/',\n",
    "    'bronze_path': f'abfss://bronze@{STORAGE_ACCOUNT}.dfs.core.windows.net/delta/',\n",
    "    'checkpoint_path': f'abfss://bronze@{STORAGE_ACCOUNT}.dfs.core.windows.net/checkpoints/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3be59399-e082-4b3f-b458-716d66c112fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create directories\n",
    "for path in CONFIG.values():\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(path)\n",
    "        print(f\"‚úÖ Created/verified: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Directory may already exist: {path}\")\n",
    "\n",
    "# Database setup\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ecommerce_bronze\")\n",
    "spark.sql(\"USE ecommerce_bronze\")\n",
    "\n",
    "print(\"‚úÖ Azure paths and database configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd67ec5-3293-4040-9306-b45bb4c73c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schemas for data validation\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"segment\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"is_active\", StringType(), True)\n",
    "])\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"subcategory\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"cost\", StringType(), True),\n",
    "    StructField(\"stock_quantity\", StringType(), True),\n",
    "    StructField(\"is_active\", StringType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"subtotal\", StringType(), True),\n",
    "    StructField(\"shipping_cost\", StringType(), True),\n",
    "    StructField(\"tax_amount\", StringType(), True),\n",
    "    StructField(\"total_amount\", StringType(), True),\n",
    "    StructField(\"currency\", StringType(), True)\n",
    "])\n",
    "\n",
    "order_items_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"unit_price\", StringType(), True),\n",
    "    StructField(\"discount_percent\", StringType(), True),\n",
    "    StructField(\"line_total\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Schemas defined for data validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ef81206-8b09-47c9-8257-be19b152d52a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest customers to Bronze container\n",
    "print(\"üîÑ Processing customers...\")\n",
    "\n",
    "df_customers = (spark.read\n",
    "                .format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .schema(customers_schema)\n",
    "                .load(f\"{CONFIG['source_path']}customers\"))\n",
    "\n",
    "# Add metadata and type casting\n",
    "df_customers = (df_customers\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"registration_date\", F.to_date(F.col(\"registration_date\")))\n",
    "    .withColumn(\"is_active\", F.col(\"is_active\").cast(\"boolean\"))\n",
    "    .withColumn(\"_source_system\", F.lit(\"ecommerce_csv\"))\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"validated\"))\n",
    ")\n",
    "\n",
    "# Save to Bronze Delta table in Azure Storage\n",
    "customers_path = f\"{CONFIG['bronze_path']}customers\"\n",
    "df_customers.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(customers_path)\n",
    "\n",
    "# Create table in metastore\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecommerce_bronze.customers\n",
    "USING DELTA\n",
    "LOCATION '{customers_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Customers: {df_customers.count()} records ingested to {customers_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b265944d-5493-465a-8697-28786510aaf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest products to Bronze container\n",
    "print(\"üîÑ Processing products...\")\n",
    "\n",
    "df_products = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .schema(products_schema)\n",
    "               .load(f\"{CONFIG['source_path']}products\"))\n",
    "\n",
    "# Add metadata and type casting\n",
    "df_products = (df_products\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"price\", F.col(\"price\").cast(\"double\"))\n",
    "    .withColumn(\"cost\", F.col(\"cost\").cast(\"double\"))\n",
    "    .withColumn(\"stock_quantity\", F.col(\"stock_quantity\").cast(\"int\"))\n",
    "    .withColumn(\"is_active\", F.col(\"is_active\").cast(\"boolean\"))\n",
    "    .withColumn(\"_source_system\", F.lit(\"ecommerce_csv\"))\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"validated\"))\n",
    ")\n",
    "\n",
    "# Save to Bronze Delta table\n",
    "products_path = f\"{CONFIG['bronze_path']}products\"\n",
    "df_products.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(products_path)\n",
    "\n",
    "# Create table in metastore\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecommerce_bronze.products\n",
    "USING DELTA\n",
    "LOCATION '{products_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Products: {df_products.count()} records ingested to {products_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b0e3b54-3939-4545-8476-095bbdd85297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest orders to Bronze container\n",
    "print(\"üîÑ Processing orders...\")\n",
    "\n",
    "df_orders = (spark.read\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", \"true\")\n",
    "             .schema(orders_schema)\n",
    "             .load(f\"{CONFIG['source_path']}orders\"))\n",
    "\n",
    "# Add metadata and type casting\n",
    "df_orders = (df_orders\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_date\")))\n",
    "    .withColumn(\"order_timestamp\", F.to_timestamp(F.col(\"order_timestamp\")))\n",
    "    .withColumn(\"subtotal\", F.col(\"subtotal\").cast(\"double\"))\n",
    "    .withColumn(\"shipping_cost\", F.col(\"shipping_cost\").cast(\"double\"))\n",
    "    .withColumn(\"tax_amount\", F.col(\"tax_amount\").cast(\"double\"))\n",
    "    .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"double\"))\n",
    "    .withColumn(\"_source_system\", F.lit(\"ecommerce_csv\"))\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"validated\"))\n",
    ")\n",
    "\n",
    "# Save to Bronze Delta table\n",
    "orders_path = f\"{CONFIG['bronze_path']}orders\"\n",
    "df_orders.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(orders_path)\n",
    "\n",
    "# Create table in metastore\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecommerce_bronze.orders\n",
    "USING DELTA\n",
    "LOCATION '{orders_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Orders: {df_orders.count()} records ingested to {orders_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9022eecc-05e5-426a-8233-97c138bd5eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest order items to Bronze container\n",
    "print(\"üîÑ Processing order items...\")\n",
    "\n",
    "df_order_items = (spark.read\n",
    "                  .format(\"csv\")\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .schema(order_items_schema)\n",
    "                  .load(f\"{CONFIG['source_path']}order_items\"))\n",
    "\n",
    "# Add metadata and type casting\n",
    "df_order_items = (df_order_items\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
    "    .withColumn(\"discount_percent\", F.col(\"discount_percent\").cast(\"double\"))\n",
    "    .withColumn(\"line_total\", F.col(\"line_total\").cast(\"double\"))\n",
    "    .withColumn(\"_source_system\", F.lit(\"ecommerce_csv\"))\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"validated\"))\n",
    ")\n",
    "\n",
    "# Save to Bronze Delta table\n",
    "order_items_path = f\"{CONFIG['bronze_path']}order_items\"\n",
    "df_order_items.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(order_items_path)\n",
    "\n",
    "# Create table in metastore\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecommerce_bronze.order_items\n",
    "USING DELTA\n",
    "LOCATION '{order_items_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Order Items: {df_order_items.count()} records ingested to {order_items_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d0eab68-9fec-445e-82c2-8267f6e424c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Quality Validation\n",
    "print(\"üîç BRONZE LAYER DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check record counts\n",
    "customers_count = spark.table('ecommerce_bronze.customers').count()\n",
    "products_count = spark.table('ecommerce_bronze.products').count()\n",
    "orders_count = spark.table('ecommerce_bronze.orders').count()\n",
    "order_items_count = spark.table('ecommerce_bronze.order_items').count()\n",
    "\n",
    "print(f\"üìä Record Counts:\")\n",
    "print(f\"   Customers: {customers_count:,}\")\n",
    "print(f\"   Products: {products_count:,}\")\n",
    "print(f\"   Orders: {orders_count:,}\")\n",
    "print(f\"   Order Items: {order_items_count:,}\")\n",
    "\n",
    "# Data quality checks\n",
    "null_customer_ids = spark.table('ecommerce_bronze.customers').filter(F.col('customer_id').isNull()).count()\n",
    "negative_prices = spark.table('ecommerce_bronze.products').filter(F.col('price') < 0).count()\n",
    "null_order_totals = spark.table('ecommerce_bronze.orders').filter(F.col('total_amount').isNull()).count()\n",
    "\n",
    "print(f\"\\nüîç Data Quality Metrics:\")\n",
    "print(f\"   Null customer IDs: {null_customer_ids}\")\n",
    "print(f\"   Negative prices: {negative_prices}\")\n",
    "print(f\"   Null order totals: {null_order_totals}\")\n",
    "\n",
    "# Calculate data quality score\n",
    "total_issues = null_customer_ids + negative_prices + null_order_totals\n",
    "total_records = customers_count + products_count + orders_count + order_items_count\n",
    "\n",
    "if total_issues == 0:\n",
    "    quality_score = 100\n",
    "    print(f\"\\n‚úÖ DATA QUALITY SCORE: {quality_score}% (Perfect)\")\n",
    "else:\n",
    "    quality_score = max(0, 100 - (total_issues / total_records * 100))\n",
    "    print(f\"\\n‚ö†Ô∏è DATA QUALITY SCORE: {quality_score:.1f}%\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Performance Optimization\n",
    "print(\"‚ö° OPTIMIZING BRONZE TABLES\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Optimize all tables for better query performance\n",
    "tables = ['customers', 'products', 'orders', 'order_items']\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"üîß Optimizing ecommerce_bronze.{table}...\")\n",
    "    spark.sql(f\"OPTIMIZE ecommerce_bronze.{table}\")\n",
    "    \n",
    "# Z-order optimization for frequently queried columns\n",
    "spark.sql(\"OPTIMIZE ecommerce_bronze.orders ZORDER BY (order_date, customer_id)\")\n",
    "spark.sql(\"OPTIMIZE ecommerce_bronze.customers ZORDER BY (country, segment)\")\n",
    "spark.sql(\"OPTIMIZE ecommerce_bronze.products ZORDER BY (category)\")\n",
    "\n",
    "print(\"‚úÖ All Bronze tables optimized\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Bronze Layer Summary and Task Values\n",
    "bronze_ingestion_timestamp = str(datetime.now())\n",
    "\n",
    "# Set task values for Silver layer\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_customers_count\", value=customers_count)\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_products_count\", value=products_count)\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_orders_count\", value=orders_count)\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_order_items_count\", value=order_items_count)\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_data_quality_score\", value=float(quality_score))\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_total_issues\", value=total_issues)\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_ingestion_timestamp\", value=bronze_ingestion_timestamp)\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_container_path\", value=f\"abfss://bronze@{STORAGE_ACCOUNT}.dfs.core.windows.net/\")\n",
    "dbutils.jobs.taskValues.set(key=\"STORAGE_ACCOUNT\", value=STORAGE_ACCOUNT)\n",
    "\n",
    "# Validation against expected volumes\n",
    "if expected_customers:\n",
    "    validation_results = {\n",
    "        \"customers_match\": customers_count == expected_customers,\n",
    "        \"products_match\": products_count == expected_products,\n",
    "        \"orders_match\": orders_count == expected_orders,\n",
    "        \"order_items_match\": order_items_count == expected_order_items\n",
    "    }\n",
    "    \n",
    "    all_validations_passed = all(validation_results.values())\n",
    "    dbutils.jobs.taskValues.set(key=\"bronze_validation_passed\", value=all_validations_passed)\n",
    "    \n",
    "    print(f\"\\nüîç Data Volume Validation:\")\n",
    "    for check, result in validation_results.items():\n",
    "        status = \"‚úÖ\" if result else \"‚ùå\"\n",
    "        print(f\"   {status} {check}: {result}\")\n",
    "\n",
    "print(\"üìã BRONZE LAYER SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìÅ Storage Location: abfss://bronze@{STORAGE_ACCOUNT}.dfs.core.windows.net/\")\n",
    "print(f\"üìä Total Records Processed: {total_records:,}\")\n",
    "print(f\"üéØ Data Quality Score: {quality_score:.1f}%\")\n",
    "print(f\"üíæ Delta Tables Created: {len(tables)}\")\n",
    "print(\"‚ö° Performance Optimizations: Applied\")\n",
    "\n",
    "print(\"\\n‚úÖ Bronze layer ingestion complete!\")\n",
    "print(\"üìÅ Data stored in Azure Bronze container\")\n",
    "print(\"üìã Task values set for Silver layer processing\")\n",
    "print(\"üîú Next: Run 03_silver_processing.py\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
